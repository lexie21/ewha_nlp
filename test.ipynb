{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "api_key = \"\" # insert upstage api key here\n",
    "data_path = \".\" # folder path containing ewah.pdf and samples.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting pdf to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "# from langchain.docstore.document import Document  # Import Document from langchain\n",
    "\n",
    "# # Load the PDF file\n",
    "# pdf_path = \"ewha.pdf\"  # Replace with your file path\n",
    "# with pdfplumber.open(pdf_path) as pdf:\n",
    "#     pdf_text = \"\"\n",
    "#     for page in pdf.pages:\n",
    "#         pdf_text += page.extract_text()\n",
    "# pdf_text = [Document(page_content=pdf_text)]\n",
    "# print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "import os\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(api_key=api_key,file_path=os.path.join(data_path, 'ewha.pdf'), output_type=\"text\")\n",
    "\n",
    "pdf_text = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "articles = text_splitter.split_documents(pdf_text)\n",
    "print(\"Splits:\", len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data of testing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read samples.csv file\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    prompts = data['prompts'][:34]\n",
    "    answers = data['answers'][:34]\n",
    "    evidences = data['evidence'][:34]\n",
    "    # returns three lists: prompts, answers and evidences\n",
    "    return prompts, answers, evidences\n",
    "\n",
    "prompts, answers, evidences = read_data(os.path.join(data_path, 'testewha.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klue robertabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load a Korean language model\n",
    "model_name = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "\n",
    "contexts = []\n",
    "count = 1\n",
    "\n",
    "# Compute embeddings\n",
    "article_embeddings = [get_embedding(article.page_content) for article in articles[:-1]]\n",
    "\n",
    "for question in prompts:\n",
    "    print(f\"Processing question {count}\")\n",
    "    count += 1\n",
    "    question_embedding = get_embedding(question)\n",
    "\n",
    "    # Compute similarities\n",
    "    similarities = [cosine_similarity(question_embedding.detach().numpy(), \n",
    "                                    article_embedding.detach().numpy())[0][0]\n",
    "                    for article_embedding in article_embeddings]\n",
    "\n",
    "    best_match = similarities.index(max(similarities))\n",
    "\n",
    "    # Get the indexes sorted by values in descending order\n",
    "    sorted_indexes = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)\n",
    "\n",
    "    # Take top 3 chunks as context\n",
    "    context = [articles[sorted_indexes[0]].page_content, articles[sorted_indexes[1]].page_content, articles[sorted_indexes[2]].page_content]\n",
    "    contexts.append(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upstage embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    " \n",
    "embeddings = UpstageEmbeddings(\n",
    "    api_key = api_key,\n",
    "    model=\"embedding-query\"\n",
    ")\n",
    "\n",
    "# Compute document embeddings\n",
    "doc_result = embeddings.embed_documents(\n",
    "    [article.page_content for article in articles]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "contexts = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    query_result = embeddings.embed_query(prompt)\n",
    "    similarity_list = []\n",
    "    for passage_embedding in doc_result:\n",
    "        similarity = np.dot(passage_embedding, query_result)\n",
    "        similarity_list.append(similarity)\n",
    "        \n",
    "    values = similarity_list\n",
    "    # Get the indexes sorted by values in descending order\n",
    "    sorted_indexes = sorted(range(len(values)), key=lambda i: values[i], reverse=True)\n",
    "\n",
    "    # Take top 3 chunks as context\n",
    "    context = [articles[sorted_indexes[0]].page_content, articles[sorted_indexes[1]].page_content, articles[sorted_indexes[2]].page_content]\n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_transformers import get_kobert_model, get_tokenizer\n",
    "import torch\n",
    "\n",
    "# Load KoBERT tokenizer and model\n",
    "tokenizer = get_tokenizer()\n",
    "model = get_kobert_model()\n",
    "\n",
    "def embed_text(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Embeds text using KoBERT.\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Mean-pool embeddings\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Embed articles\n",
    "article_embeddings = torch.vstack([embed_text(article.page_content, tokenizer, model) for article in articles])\n",
    "contexts = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # Embed question\n",
    "    question_embedding = embed_text(prompt, tokenizer, model)\n",
    "\n",
    "    # Compute cosine similarity between question and articles\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(article_embeddings, question_embedding)\n",
    "\n",
    "    # Find the top 3 most relevant articles\n",
    "    values = cosine_sim\n",
    "    # Get the indexes sorted by values in descending order\n",
    "    sorted_indexes = sorted(range(len(values)), key=lambda i: values[i], reverse=True)\n",
    "\n",
    "    # Take top 3 chunks as context\n",
    "    context = [articles[sorted_indexes[0]].page_content, articles[sorted_indexes[1]].page_content, articles[sorted_indexes[2]].page_content]\n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract keywords and match using Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import  Okt\n",
    "\n",
    "# Example question\n",
    "korean_question = prompts[0]\n",
    "\n",
    "# Initialize a tagger (Okt)\n",
    "okt = Okt()\n",
    "\n",
    "# Tokenize and extract nouns (keywords)\n",
    "okt_keywords = okt.nouns(korean_question)\n",
    "\n",
    "print(korean_question)\n",
    "print(\"Okt Keywords:\", okt_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords from each article\n",
    "article_keywords = [okt.nouns(article.page_content) for article in articles]\n",
    "contexts = []\n",
    "\n",
    "for prompt in prompts:\n",
    "\n",
    "    okt_keywords = okt.nouns(prompt)\n",
    "    # Convert keywords to sets for easy comparison\n",
    "    question_keywords = set(okt_keywords)\n",
    "\n",
    "    # Calculate relevance scores based on keyword overlap\n",
    "    relevance_scores = [\n",
    "        len(question_keywords.intersection(set(article_keywords)))\n",
    "        for article_keywords in article_keywords\n",
    "    ]\n",
    "\n",
    "    # Get the indexes of the top articles\n",
    "    top_indexes = sorted(\n",
    "        range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True\n",
    "    )\n",
    "\n",
    "    # Take top 3 chunks as context\n",
    "    context = [articles[top_indexes[0]].page_content, articles[top_indexes[1]].page_content, articles[top_indexes[2]].page_content]\n",
    "    contexts.append(context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage(api_key = api_key, model=\"solar-1-mini-chat\")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer from the following context. Select an option from the choices provided and answer with a single letter.\n",
    "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "responses = []\n",
    "count = 0\n",
    "\n",
    "for prompt in prompts: \n",
    "    print(f\"Processing question {count+1}\")\n",
    "    response = chain.invoke({\"question\": prompt, \"context\": \"\\n\".join(contexts[count])})\n",
    "    responses.append(response.content)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for response in responses:\n",
    "    print(f\"Question {count+1} : {response} \\t Correct answer: {answers[count]}\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for response in responses:\n",
    "    print(f\"Question {count+1} : {response} \\t Correct answer: {answers[count]} \\t Context: {evidences[count]}\")\n",
    "    print(f\"Question: {prompts[count]}\")\n",
    "    print(f\"Context 1: {contexts[count][0]}\")\n",
    "    print(f\"Context 2: {contexts[count][1]}\")\n",
    "    print(f\"Context 3: {contexts[count][2]}\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMLU without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read samples.csv file\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    prompts = data['prompts']\n",
    "    answers = data['answers']\n",
    "    # returns two lists: prompts and answers\n",
    "    return prompts, answers\n",
    "\n",
    "prompts, answers = read_data(os.path.join(data_path, 'test_samples_MMLU-LAW.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage(api_key = api_key,model=\"solar-1-mini-chat\")\n",
    "\n",
    "responses = []\n",
    "count = 1\n",
    "\n",
    "print(f\"Processing question {count}\")\n",
    "count += 1\n",
    "messages = [\n",
    "HumanMessage(\n",
    "    content = \"QUESTION1) A woman was standing in the aisle of a subway car and put her purse on the seat next to her. A man approached the woman from behind and grabbed the purse off the seat. He then pushed the woman out of the way and ran out of the subway car while carrying the purse. The man was apprehended on the subway platform while in possession of the purse. In a jurisdiction that follows the common law with respect to criminal offenses, of what crime can the man properly be convicted? (A) Fraud, because he took the purse without the woman's consent. (B) Larceny, because he took the purse without the woman's permission. (C) Burglary, because he entered the subway car with the intention of committing a theft. (D) Robbery, because he used force in leaving with the purse. (E) Robbery, because he used force to take possession of the purse. (F) Robbery, because he used force to remove the woman from the purse's vicinity. (G) Larceny, because force was not used until after he took the purse. (H) Assault, because he pushed the woman out of the way. (I) Larceny, because he made no threat to use force. (J) Robbery, because he physically took the purse from the woman's presence.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "responses.append(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import HumanMessage\n",
    "# from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "# llm = ChatUpstage(api_key = api_key,model=\"solar-1-mini-chat\")\n",
    "\n",
    "# responses = []\n",
    "# count = 1\n",
    "\n",
    "# for prompt in prompts:\n",
    "#     print(f\"Processing question {count}\")\n",
    "#     count += 1\n",
    "#     messages = [\n",
    "#     HumanMessage(\n",
    "#         content = \"Please answer the following question by choosing the most appropriate option from the choices provided. Keep your answer within ten words\" + prompt\n",
    "#         )\n",
    "#     ]\n",
    "\n",
    "#     response = llm.invoke(messages)\n",
    "#     responses.append(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage(api_key = api_key,model=\"solar-1-mini-chat\")\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide the most correct answer from the following context. Understand the entire full sentence for completeness.\n",
    "    Q: Who likes to eat chips? Context says that Ha, Lam, Bun like to eat chips. \n",
    "    (A): Ha \n",
    "    (B): Lam\n",
    "    (C): Bun\n",
    "    (D): All of the above\n",
    "    A: (D)\n",
    "\n",
    "    If the answer is not present in the context, please  provide a reasonable guess of the answer.\n",
    "\n",
    "    Also, once you find the answer in the context, ignore the remainder of the context.\n",
    "\n",
    "    The answer must look like this always! \n",
    "    Answer: (<answer>)\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "responses = []\n",
    "count = 1\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Processing question {count}\")\n",
    "    count += 1\n",
    "\n",
    "    response = chain.invoke({\"question\": prompt, \"context\": prompt})\n",
    "    responses.append(response.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for response in responses:\n",
    "    print(f\"Question {count+1} : {response} \\t Correct answer: {answers[count]}\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spacy to extract keywords and wikipedia api to retrieve keyword summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_keywords(question):\n",
    "    # Process the question\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    # Extract nouns, proper nouns, and compound nouns\n",
    "    keywords = [chunk.text for chunk in doc.noun_chunks]\n",
    "    return keywords\n",
    "\n",
    "# Example question\n",
    "question = \"According to social identity theory, an individual's self-concept is primarily derived from:\"\n",
    "keywords = extract_keywords(question)\n",
    "print(\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "# Create a Wikipedia API instance\n",
    "wiki = wikipediaapi.Wikipedia(\"NLP Project (yanrenyu00@gmail.com)\", \"en\")\n",
    "\n",
    "# Specify the page title\n",
    "page_title = \"social identity theory\"\n",
    "page = wiki.page(page_title)\n",
    "\n",
    "# Check if the page exists\n",
    "if page.exists():\n",
    "    print(f\"Title: {page.title}\")\n",
    "    print(f\"Summary: {page.summary}\")\n",
    "else:\n",
    "    print(f\"The page '{page_title}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building context from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import wikipediaapi\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Wikipedia API instance\n",
    "wiki = wikipediaapi.Wikipedia(\"NLP Project (yanrenyu00@gmail.com)\", \"en\")\n",
    "\n",
    "def extract_keywords(question):\n",
    "    # Process the question\n",
    "    doc = nlp(question)\n",
    "    \n",
    "    # Extract nouns, proper nouns, and compound nouns\n",
    "    keywords = [chunk.text for chunk in doc.noun_chunks]\n",
    "    return keywords\n",
    "\n",
    "count = 1\n",
    "contexts = []\n",
    "\n",
    "# Example question\n",
    "for prompt in prompts:\n",
    "\n",
    "    # Remove the prefix and slice until (A)\n",
    "    start_index = prompt.find(')')+1\n",
    "    end_index = prompt.find('(A)')\n",
    "\n",
    "    # Extract the substring\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        question = prompt[start_index:end_index].strip()\n",
    "        print(question)\n",
    "    else:\n",
    "        question = prompt\n",
    "    keywords = extract_keywords(question)\n",
    "    print(f\"Processing question {count}, keywords: {keywords}\")\n",
    "    count += 1\n",
    "\n",
    "    context = []\n",
    "    for keyword in keywords:\n",
    "        page_title = keyword\n",
    "        page = wiki.page(page_title)\n",
    "\n",
    "        # Check if the page exists\n",
    "        if page.exists():\n",
    "            context.append(page.summary)\n",
    "        else:\n",
    "            print(f\"Page does not exist for {keyword}\")\n",
    "    contexts.append(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing MMLU with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage(api_key = api_key, model=\"solar-mini\")\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer from the following context. Select an option from the choices provided and answer with a single letter.\n",
    "    Keep your answer within ten words.\n",
    "    If the answer is not present in the context, please still try you best to answer it.\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "responses = []\n",
    "count = 0\n",
    "\n",
    "for prompt in prompts: \n",
    "    print(f\"Processing question {count+1}\")\n",
    "    response = chain.invoke({\"question\": prompt, \"context\": \"\\n\".join(contexts[count])})\n",
    "    responses.append(response.content)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for response in responses:\n",
    "    print(f\"Question {count+1} : {response} \\t Correct answer: {answers[count]}\")\n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
